{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IV7E1ndRyBU1"
   },
   "source": [
    "# Assignment 5\n",
    "\n",
    "#### Christopher W. Hong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUsqQC7PQrgw"
   },
   "source": [
    "The Road Ahead\n",
    "\n",
    "* [Step 1:](#1) Sentiment classfication\n",
    "  * [Step 1a:](#1a) Load the IMDB data\n",
    "  * [Step 1b:](#1b) Split the IMDB data\n",
    "  * [Step 1c:](#1c) Build model with a trainable embedding layer two fully connected layers\n",
    "  * [Step 1d:](#1d) Train and evaluate accuracy\n",
    "  * [Step 1e:](#1e) Downloading the GLOVE word embeddings and applied\n",
    "  * [Step 1f:](#1f) Train and evaluate accuracy with embedding layer weights fixed\n",
    "  * [Step 1g:](#1g) Conclusions about the trainable and GLOVE embedding\n",
    "  * [Step 1h:](#1h) Build a model with an LSTM layer and a trainable embedding layer\n",
    "  * [Step 1i:](#1i) Train and evaluate\n",
    "  \n",
    "* [Step 2:](#2) Topic classfication\n",
    "  * [Step 2a:](#2a) Load the Reuters newswire data\n",
    "  * [Step 2b:](#2b) Split the Reuters data\n",
    "  * [Step 2c:](#2c) Build model with a trainable embedding layer and two fully connected layers\n",
    "  * [Step 2d:](#2d) Train and evaluate accuracy with both trainable and loaded embedding layer\n",
    "  * [Step 2e:](#2e) Compare results and draw conclusion\n",
    "  * [Step 2f:](#2f) Build a model with an LSTM layer and a trainable embedding layer; train, evaluate, compare and draw conclusions.\n",
    "  * [Step 2g:](#2g) Add an addtional LSTM layer to the previous model; train and evaluate.\n",
    "  * [Step 2h:](#2h) Conclusion\n",
    "\n",
    "You might wanna **skip the fist code block of 1e** that it will download the GLOVE to the current local directory. If so, please set the correct path that refers to **a folder `glove_dir` contains `glove.6B.100d.txt`** in the second code block of 1e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFQHdvhs3i2l"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb, reuters\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten, LSTM\n",
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "!pip -q install wget\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMkvSDwUyNhU"
   },
   "source": [
    "<a id='1'></a>\n",
    "## 1: Sentiment classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSfn29Wnyjf7"
   },
   "source": [
    "<a id='1a'></a>\n",
    "### (a) Load the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dh9WQicow-si",
    "outputId": "eec7bc60-269f-42b1-b547-3c67f66c6ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000 # number of features\n",
    "maxlen = 20 # input sequence length\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences\n",
    "x_train = sequence.pad_sequences(x_train, maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3Oy4uZ9zi7j"
   },
   "source": [
    "<a id='1b'></a>\n",
    "### (b) Split the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "yKdip2Y9zouj",
    "outputId": "3dcc8c61-cc62-4728-cc3d-5b04a4a7cff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 20)\n",
      "x_valid shape: (12500, 20)\n",
      "x_test shape: (12500, 20)\n"
     ]
    }
   ],
   "source": [
    "x_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test,\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=y_test)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_valid shape:', x_valid.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XpdRO0SQ3Tcz"
   },
   "source": [
    "<a id='1c'></a>\n",
    "### (c) Build model with a trainable embedding layer two fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "Rwp53s3_3LEK",
    "outputId": "0dd7afd9-9c47-46a9-ca83-17a80c5eb047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_47 (Embedding)     (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 32)                5152      \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 85,185\n",
      "Trainable params: 85,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 8 \n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQ64C8CY7_6c"
   },
   "source": [
    "<a id='1d'></a>\n",
    "### (d) Train and evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "b_1to6718JAb",
    "outputId": "8e785957-11d7-48e3-b53f-776b1e40a2bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "12500/12500 [==============================] - 0s 21us/step\n",
      "Test accuracy: 0.7159199714660645\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=[EarlyStopping(patience=5)],\n",
    "          verbose=0)\n",
    "\n",
    "_, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ykZyY2Lf8zCL"
   },
   "source": [
    "<a id='1e'></a>\n",
    "### (e) Downloading the GLOVE word embeddings and applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UbuMQ6quApad",
    "outputId": "084c7bd8-5c63-49b7-8787-56e53f26ad65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.zip downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download the GLOVE word embeddings\n",
    "if not os.path.isfile('./glove.6B.zip'):\n",
    "  print('Downloading GLOVE...')\n",
    "  !wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "  !unzip glove.6B.zip\n",
    "else:\n",
    "  print('glove.6B.zip downloaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "1Wt81nhY5t2j",
    "outputId": "944487de-90c5-450f-b3c3-219fe31eafcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_48 (Embedding)     (None, 20, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 32)                64032     \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,064,065\n",
      "Trainable params: 1,064,065\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Parse the GLOVE word-embeddings file\n",
    "# Feel free to modify the dir refered to glove.6B.100d.txt\n",
    "# Adapted fro F. Chollet\n",
    "glove_dir = './' \n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "  values = line.split()\n",
    "  word = values[0]\n",
    "  coefs = np.asarray(values[1:], dtype='float32')\n",
    "  embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "# Prepare the GLOVE word-embeddings matrix\n",
    "# Adapted fro F. Chollet\n",
    "embedding_dim = 100\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "  if i < max_features:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "# Initialize the embedding layer using the embedding matrix and freeze the weights\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y9nFMQ3i_w8P"
   },
   "source": [
    "<a id='1f'></a>\n",
    "### (f) Train and evaluate accuracy with embedding layer weights fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "s8pdM7WTDwDf",
    "outputId": "7739ec0a-7251-4e33-fa7e-35c020b9f465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "12500/12500 [==============================] - 0s 28us/step\n",
      "Test accuracy: 0.7074400186538696\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=[EarlyStopping(patience=5)],\n",
    "          verbose=0)\n",
    "\n",
    "_, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1k-BRBqFwd9"
   },
   "source": [
    "<a id='1g'></a>\n",
    "### (g) Conclusions about the trainable and GLOVE embedding\n",
    "\n",
    "\n",
    "The test accuracy of the model with GLOVE embedding layer is close to but no better than that of the one with trainable embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKMnAnnfIB-x"
   },
   "source": [
    "<a id='1h'></a>\n",
    "### (h) Build a model with an LSTM layer and a trainable embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "geNVF-ZQEMZc",
    "outputId": "3e64fcdc-e835-469a-95db-3acd69649ec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_49 (Embedding)     (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 32)                5248      \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 85,281\n",
      "Trainable params: 85,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apWeI6dtJRxX"
   },
   "source": [
    "<a id='1i'></a>\n",
    "### (i) Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "hUQlRcjUJQcB",
    "outputId": "df856d8e-0a1f-49de-be63-9b1f044679fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "12500/12500 [==============================] - 1s 81us/step\n",
      "Test accuracy: 0.7252799868583679\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=[EarlyStopping(patience=5)],\n",
    "          verbose=0)\n",
    "\n",
    "_, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gol8woyVJsN_"
   },
   "source": [
    "The test accuracy of the model with trainable embedding layer and LSTM layer is slightly higher (~2.%) than that of the one with fully connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ulfw4M5jKadW"
   },
   "source": [
    "<a id='2'></a>\n",
    "## 2: Topic classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x9-CGTv1K1JD"
   },
   "source": [
    "<a id='2a'></a>\n",
    "### (a) Load the Reuters newswire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoijsJXrKG_J"
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 500\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
    "                                                         num_words=max_features,\n",
    "                                                         skip_top=0,\n",
    "                                                         maxlen=maxlen,\n",
    "                                                         test_split=0.3,\n",
    "                                                         seed=113,\n",
    "                                                         start_char=1,\n",
    "                                                         oov_char=2,\n",
    "                                                         index_from=3)\n",
    "\n",
    "# Pad sequences\n",
    "x_train = sequence.pad_sequences(x_train, maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKTOuWFUL-Xb"
   },
   "source": [
    "<a id='2b'></a>\n",
    "### (b) Split the Reuters data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "lpjOyPbEKZOM",
    "outputId": "09c07eb4-fbe1-4800-8ea1-bc8d9bbea407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (7543, 500)\n",
      "x_valid shape: (1617, 500)\n",
      "x_test shape: (1617, 500)\n",
      "y_train shape: (7543, 46)\n",
      "y_valid shape: (1617, 46)\n",
      "y_test shape: (1617, 46)\n"
     ]
    }
   ],
   "source": [
    "x_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test,\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=y_test)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_valid = to_categorical(y_valid)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_valid shape:', x_valid.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_valid shape:', y_valid.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMW1adp1MniX"
   },
   "source": [
    "<a id='2c'></a>\n",
    "### (c) Build model with a trainable embedding layer and two fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "kAzONHu6LsQv",
    "outputId": "b487e165-ff9a-4c1f-b80d-a2ef98f05439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_46 (Embedding)     (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 64)                1024064   \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 1,347,054\n",
      "Trainable params: 1,347,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32 \n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZjCBdy5oNaOk"
   },
   "source": [
    "<a id='2d'></a>\n",
    "### (d) Train and evaluate accuracy with both trainable and loaded embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "bssDLTILLtJN",
    "outputId": "440bac05-54bc-46d5-a6c3-83085d042413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "1617/1617 [==============================] - 0s 89us/step\n",
      "Test accuracy: 0.716759443283081\n"
     ]
    }
   ],
   "source": [
    "# With trainable embedding layer\n",
    "print('Training...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=[EarlyStopping(patience=5)],\n",
    "          verbose=0)\n",
    "\n",
    "_, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "fnkJ_pWMN8Wx",
    "outputId": "c6617605-2af1-4aad-a709-d7f3fa27e000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_45 (Embedding)     (None, 500, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 50000)             0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 64)                3200064   \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 5,203,054\n",
      "Trainable params: 4,203,054\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "1617/1617 [==============================] - 0s 218us/step\n",
      "Test accuracy: 0.6035869121551514\n"
     ]
    }
   ],
   "source": [
    "# with loaded embedding layer\n",
    "# Prepare the GLOVE word-embeddings matrix\n",
    "embedding_dim = 100\n",
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "  if i < max_features:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "# Initialize the embedding layer using the embedding matrix and freeze the weights\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Training...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=[EarlyStopping(patience=5)],\n",
    "          verbose=0)\n",
    "\n",
    "_, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvzkftVnSYGC"
   },
   "source": [
    "<a id='2e'></a>\n",
    "### (e) Compare results and draw conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ulspw5rTSgRG"
   },
   "source": [
    "The test accuracy of the model with trainable embedding layer and 2 fully connected layers is much higher (~13%) than that of the one with loaded embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E8TrdN98TS0C"
   },
   "source": [
    "<a id='2f'></a>\n",
    "### (f) Build a model with an LSTM layer and a trainable embedding layer; train, evaluate, compare and draw conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "A7o4u_rOSdOD",
    "outputId": "f66dd5db-0406-4768-f30d-b8b9a1532ac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_44 (Embedding)     (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 347,822\n",
      "Trainable params: 347,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a model with an LSTM layer and a trainable embedding layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CM7LH7xeT09a",
    "outputId": "50de107c-e4d5-4f2a-ba0d-5fb63daf3b26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "1617/1617 [==============================] - 3s 2ms/step\n",
      "Test accuracy: 0.6561533808708191\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "print('Training...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=[EarlyStopping(patience=5)],\n",
    "          verbose=0)\n",
    "\n",
    "_, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JdSBoMbXUUBf"
   },
   "source": [
    "<a id='2g'></a>\n",
    "\n",
    "### (g) Add an addtional LSTM layer to the previous model; train and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "_Il5pjTmUE2o",
    "outputId": "c6819cbf-9b69-4025-8a03-52a0fd54d714"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_43 (Embedding)     (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 500, 64)           24832     \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 380,846\n",
      "Trainable params: 380,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "1617/1617 [==============================] - 6s 4ms/step\n",
      "Test accuracy: 0.6171923279762268\n"
     ]
    }
   ],
   "source": [
    "# Build a model with two LSTM layers and a trainable embedding layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train and evaluate\n",
    "print('Training...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=[EarlyStopping(patience=5)],\n",
    "          verbose=0)\n",
    "\n",
    "_, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpOd5wGeWbaX"
   },
   "source": [
    "<a id='2h'></a>\n",
    "\n",
    "### (h) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-jO9YRkA7QvM"
   },
   "source": [
    "The model with trainable embedding layer and 2 fully connected layers had the highest accuracy over others. In topic classification, the number of rare words that belong to a certain topic can distinguish it well from other topics. Thus, the order of words in the topic is not as important as those in the sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8P8Qci4P_Js"
   },
   "source": [
    "## References\n",
    "\n",
    "Chollet, F. (2018). Deep learning with python (1st ed.). Shelter Island, NY: Manning Publications Co."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment5 Programming Source Code.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
