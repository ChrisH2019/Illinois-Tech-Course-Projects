{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import libraries as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import binom\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Download and uncompress datasets\n",
    "\n",
    "The datasets being used are as follow:\n",
    "* `Pang & Lee's movie review dataset “polarity dataset v2.0”, review_polarity.tar.gz, ` consists of `'positive'` and `'negative'` movie reviews that are stored in `pos` and `neg` directories, subdirectories of `txt_sentoken` of `review_polarity`.\n",
    "\n",
    "* `Bing Liu’s sentiment lexicon dataset “opinion lexicon”, opinion-lexicon-English.rar, ` consists of `positive-words` and `negative-words` `.txt` files in `opinion-lexicon-English` directory.\n",
    "\n",
    "* Link to [polarity dataset v2.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz)\n",
    "\n",
    "* Link to [opinion lexicon](http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)\n",
    "\n",
    "* For `Mac` users, `The Unarchiver` is one of the apps that is used to uncompress `.rar` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Exploring Bing Liu's sentiment lexicon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment_lexicon(file_dir):\n",
    "    \"\"\"\n",
    "    Extract and tokenize Bing Liu's sentiment lexicon.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a sentiment lexicon dictionary\n",
    "        sentiment_lexicon = {}\n",
    "        with open(file_dir, errors='ignore') as f:\n",
    "            # Read all the lines\n",
    "            lines = f.readlines() \n",
    "            for line in lines:\n",
    "                # Normalize the case and strip the lines\n",
    "                line = line.lower().strip() \n",
    "                # Remove punctuation characters\n",
    "                line = re.sub(r'[^\\w\\+\\*\\-]', ' ', line)\n",
    "                # Extract a single word in a line\n",
    "                if len(line.split(' ')) == 1 and line.split(' ')[0] != '':\n",
    "                    sentiment_lexicon[line] = len(sentiment_lexicon)\n",
    "        return sentiment_lexicon\n",
    "    except:\n",
    "        print('Extract data failed. Please check data directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of positive sentiment lexicons:\n",
      " ['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation']\n",
      "\n",
      "Overview of negative sentiment lexicons:\n",
      " ['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted']\n"
     ]
    }
   ],
   "source": [
    "# File path refer to Bing Liu’s positive sentiment lexicon\n",
    "pos_sentiment_lexicon_dir = './opinion-lexicon-English/positive-words.txt'\n",
    "pos_sentiment_lexicon = extract_sentiment_lexicon(pos_sentiment_lexicon_dir)\n",
    "print('Overview of positive sentiment lexicons:\\n', \n",
    "      list(pos_sentiment_lexicon)[:10])\n",
    "\n",
    "# File path refer to Bing Liu’s negative sentiment lexicon\n",
    "neg_sentiment_lexicon_dir = './opinion-lexicon-English/negative-words.txt'\n",
    "neg_sentiment_lexicon = extract_sentiment_lexicon(neg_sentiment_lexicon_dir)\n",
    "print('\\nOverview of negative sentiment lexicons:\\n', \n",
    "      list(neg_sentiment_lexicon)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of positive sentiment lexicon: 2006 words\n",
      "Vocabulary of negative sentiment lexicon: 4783 words\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary of positive sentiment lexicon:', \n",
    "      len(pos_sentiment_lexicon.keys()), 'words')\n",
    "print('Vocabulary of negative sentiment lexicon:', \n",
    "      len(neg_sentiment_lexicon.keys()), 'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Exploring Pang & Lee's movie review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_movie_review_data(data_dir='./review_polarity/txt_sentoken'):\n",
    "    \"\"\"\n",
    "    Extract moview review data from given directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Data and labels to be returned\n",
    "        data = []\n",
    "        labels = []\n",
    "\n",
    "        # Assume 2 sub-directories: neg, pos\n",
    "        for sentiment in ['neg', 'pos']:\n",
    "            dir_name = os.path.join(data_dir, sentiment)\n",
    "            for fname in os.listdir(dir_name):\n",
    "                if fname[-4:] == '.txt':\n",
    "                    with open(os.path.join(dir_name, fname)) as f:\n",
    "                        data.append(f.read())\n",
    "                        if sentiment == 'neg':\n",
    "                            labels.append(0)\n",
    "                        else:\n",
    "                            labels.append(1)\n",
    "        return np.array(data), np.array(labels)\n",
    "    except:\n",
    "        print('Read data failed. Please check data directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moive reviews: 1000 positive and 1000 negative\n"
     ]
    }
   ],
   "source": [
    "# Read movie review data\n",
    "data, labels = read_movie_review_data()\n",
    "print('Moive reviews: {} positive and {} negative'.format(\n",
    "    len(labels) - np.sum(labels), len(labels) - np.sum(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick look at one of the positive reviews:\n",
      "\n",
      "the happy bastard's 30-second review : \n",
      "american pie \n",
      "the summer of raunch continues to spread into theatres with this latest yuk fest , filled with sick jokes and teen dialogue aplenty . \n",
      "if you go expecting dawson's creek , you're in for a problem . \n",
      "if your expectations are lower ( and better , i might add ) , you will enjoy the hell out of american pie . \n",
      "the movie casts several unknowns , with the only real recognizable one being sctv's own eugene levy as a happy-go-lucky dad . \n",
      "the story revolves around four high school seniors who have one goal before the school year gets out- get laid . \n",
      "that's pretty much it . \n",
      "throughout the movie , little sick comic bits are sprinkled throughout , including a memorable scene involving an apple pie ( i won't give it away , but you probably know what it is ) and an internet broadcast gone horribly awry . \n",
      "of course , the movie has some slightly sentimental bits , but they don't drag the movie's humor content down that bad . \n",
      "most of the actors get their job done , but it's levy who's a hoot , a father who tries to talk sex with his son with the help of some curious \" visual aids \" . \n",
      "i couldn't stop laughing during this movie , and if you can stand all the raunch and the sex references , then american pie is for you . \n",
      "if you're one of those \" conservative \" types , well , i'll bet you're having fun at home while this , south park , and austin powers ii plays in theatres , now aren't you ? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('A quick look at one of the positive reviews:\\n')\n",
    "print(data[1400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick look at one of the negative reviews:\n",
      "\n",
      "the happy bastard's quick movie review \n",
      "damn that y2k bug . \n",
      "it's got a head start in this movie starring jamie lee curtis and another baldwin brother ( william this time ) in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on . \n",
      "little do they know the power within . . . \n",
      "going for the gore and bringing on a few action sequences here and there , virus still feels very empty , like a movie going for all flash and no substance . \n",
      "we don't know why the crew was really out in the middle of nowhere , we don't know the origin of what took over the ship ( just that a big pink flashy thing hit the mir ) , and , of course , we don't know why donald sutherland is stumbling around drunkenly throughout . \n",
      "here , it's just \" hey , let's chase these people around with some robots \" . \n",
      "the acting is below average , even from the likes of curtis . \n",
      "you're more likely to get a kick out of her work in halloween h20 . \n",
      "sutherland is wasted and baldwin , well , he's acting like a baldwin , of course . \n",
      "the real star here are stan winston's robot design , some schnazzy cgi , and the occasional good gore shot , like picking into someone's brain . \n",
      "so , if robots and body parts really turn you on , here's your movie . \n",
      "otherwise , it's pretty much a sunken ship of a movie . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('A quick look at one of the negative reviews:\\n')\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Preprocess movie review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(data):\n",
    "    \"\"\"\n",
    "    Convert a raw text review into a sequence of words.\n",
    "    \"\"\"\n",
    "    # Remove punctuation characters\n",
    "    data_normalized = []\n",
    "    for text in data:\n",
    "        # Convert to lowercase and remove certain punctuation characters\n",
    "        tmp_text = re.sub(r'[^\\w\\'\\+\\*\\-/]', ' ', text.lower())\n",
    "        data_normalized.append(' '.join(tmp_text.split()))\n",
    "\n",
    "    return np.array(data_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick look at one of the normalized reviews:\n",
      "\n",
      " the happy bastard's quick movie review damn that y2k bug it's got a head start in this movie starring jamie lee curtis and another baldwin brother william this time in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on little do they know the power within going for the gore and bringing on a few action sequences here and there virus still feels very empty like a movie going for all flash and no substance we don't know why the crew was really out in the middle of nowhere we don't know the origin of what took over the ship just that a big pink flashy thing hit the mir and of course we don't know why donald sutherland is stumbling around drunkenly throughout here it's just hey let's chase these people around with some robots the acting is below average even from the likes of curtis you're more likely to get a kick out of her work in halloween h20 sutherland is wasted and baldwin well he's acting like a baldwin of course the real star here are stan winston's robot design some schnazzy cgi and the occasional good gore shot like picking into someone's brain so if robots and body parts really turn you on here's your movie otherwise it's pretty much a sunken ship of a movie\n"
     ]
    }
   ],
   "source": [
    "data_normalized = review_to_words(data)\n",
    "print('A quick look at one of the normalized reviews:\\n\\n', data_normalized[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Form train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie reviews: \n",
      "  train data = 1600, positive train labels = 800, negative train labels = 800\n",
      "  test data = 400, positive test labels = 200, negative test labels = 200\n"
     ]
    }
   ],
   "source": [
    "# 8/2 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_normalized,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=labels)\n",
    "\n",
    "print(\"\"\"Movie reviews: \n",
    "  train data = {}, positive train labels = {}, negative train labels = {}\n",
    "  test data = {}, positive test labels = {}, negative test labels = {}\"\"\".\n",
    "      format(len(X_train), np.sum(y_train), len(y_train) - np.sum(y_train),\n",
    "             len(X_test), np.sum(y_test), len(y_test) - np.sum(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Compute Bag-of-Words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# Fit the training data and return the matrix\n",
    "X_train_vectorized = count_vector.fit_transform(X_train).toarray()\n",
    "\n",
    "# Transofmr the testing data and return the marix\n",
    "X_test_vectorized = count_vector.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of movie reviews: 36180\n",
      "\n",
      "Sample words: ['still', 'can', 'figure', 'out', 'why', 'people', 'went', 'in', 'droves', 'to', 'see', 'this', 'movie', 'now', 'before', 'you', 'go', 'assuming', 'some', 'sort', 'of', 'high', 'brow', 'snob', 'who', 'appreciate', 'little', 'dumb', 'humor', 'let']\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary of movie reviews:', len(count_vector.vocabulary_.keys()))\n",
    "print('\\nSample words:', list(count_vector.vocabulary_.keys())[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Classification using lexicon-based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_based_clf(X_test, pos, neg):\n",
    "    \"\"\"\n",
    "    Implementation of the lexicon-based classifier mentioned by J. Eisenstein.\n",
    "    Classify each document in texts as positive \n",
    "    iff it has more positive sentiment words \n",
    "    than negative sentiment words.\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    for text in X_test:\n",
    "        pos_count = 0 # number of positive sentiment lexicons\n",
    "        neg_count = 0 # number of negative sentiment lexicons\n",
    "        for word in text.split():\n",
    "            if word in pos:\n",
    "                pos_count += 1\n",
    "            if word in neg:\n",
    "                neg_count += 1\n",
    "        if pos_count > neg_count:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "            \n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon-based classifier performance:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.74      0.71       200\n",
      "    positive       0.72      0.67      0.69       200\n",
      "\n",
      "    accuracy                           0.70       400\n",
      "   macro avg       0.70      0.70      0.70       400\n",
      "weighted avg       0.70      0.70      0.70       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_lb = lexicon_based_clf(X_test, \n",
    "                           pos_sentiment_lexicon, \n",
    "                           neg_sentiment_lexicon)\n",
    "\n",
    "print('Lexicon-based classifier performance:\\n')\n",
    "print(classification_report(y_test, \n",
    "                            y_pred_lb, \n",
    "                            target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Classification using Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression classifier\n",
    "logistic_clf = LogisticRegression(random_state=0).fit(X_train_vectorized, \n",
    "                                                      y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier performance:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85       200\n",
      "    positive       0.85      0.85      0.85       200\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.85      0.85      0.85       400\n",
      "weighted avg       0.85      0.85      0.85       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_lg = logistic_clf.predict(X_test_vectorized)\n",
    "\n",
    "print('Logistic regression classifier performance:\\n')\n",
    "print(classification_report(y_test, \n",
    "                            y_pred_lg, \n",
    "                            target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Determine whether the differences of the performances are significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The binomial test on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances disagree on: 124\n"
     ]
    }
   ],
   "source": [
    "# N instances in a test set that \n",
    "# lexicon-based classifier and logistic regression classifier disagree on\n",
    "disagrees = ~(y_pred_lb == y_pred_lg)\n",
    "N = np.sum(disagrees)\n",
    "print('Number of instances disagree on:', N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances that lexicon-based classifier is correct on: 59\n"
     ]
    }
   ],
   "source": [
    "# k instances in N instances that lexicon-based classifier is correct on\n",
    "corrects_lb = (y_pred_lb == y_test)\n",
    "correct_lb_on_disagrees = (corrects_lb == disagrees)\n",
    "k = np.sum(correct_lb_on_disagrees)\n",
    "print('Number of instances that lexicon-based classifier is correct on:', k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value: 0.12382904545780314\n",
      "p_value < 0.05: False\n"
     ]
    }
   ],
   "source": [
    "# Compute a two-tailed p-value\n",
    "p = 0.5 # binary class\n",
    "p_val = binom.pmf(k, N, p) * 2 # two-tailed\n",
    "print('p_value:', p_val)\n",
    "print('p_value < 0.05:', p_val < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the `two-tailed p-value`, lexicon-based classifier and logistic regression classifier are equally accurate in terms of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### The bootstrap test on macro-F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1000/1000 boostrap samples"
     ]
    }
   ],
   "source": [
    "# number of bootstrap samples\n",
    "M = 1000 \n",
    "\n",
    "# number of test instances\n",
    "N = len(X_test) \n",
    "\n",
    "# Matrix to store the difference of f-measure\n",
    "delta_f_measure = np.zeros((M, 1)) \n",
    "\n",
    "# Implementation of bootstrap-sample procedure mentioned by J. Eisenstein.\n",
    "for m in range(M):\n",
    "    # Resample instances from the test set with replacement\n",
    "    resample_index = np.random.choice(np.random.randint(0, N, N), \n",
    "                                      size=N, \n",
    "                                      replace=True)\n",
    "    \n",
    "    # Predictions of lexicon-based classifier on boostrap sample\n",
    "    y_pred_lb = lexicon_based_clf(X_test[resample_index], \n",
    "                                       pos_sentiment_lexicon, \n",
    "                                       neg_sentiment_lexicon)\n",
    "    \n",
    "    # Predictions of logistic regression classifier on boostrap sample\n",
    "    y_pred_lg = logistic_clf.predict(X_test_vectorized[resample_index])\n",
    "    \n",
    "    # Ground true labels of the boostrap sample\n",
    "    y_true = y_test[resample_index]\n",
    "    \n",
    "    # f-meausre score of lexicon-based classifier\n",
    "    f_measure_lb = f1_score(y_true, y_pred_lb)\n",
    "    \n",
    "    # f-measure score of logistic regression classifier\n",
    "    f_measure_lg = f1_score(y_true, y_pred_lg)\n",
    "    \n",
    "    # Difference of f-measure between lexicon-based and logistic classifiers\n",
    "    delta_f_measure[m, 0] = f_measure_lb - f_measure_lg\n",
    "    \n",
    "    # Monitor the progress of bootstrap samples\n",
    "    sys.stdout.write(\"\\rProgress: {}/{} boostrap samples\".format(m+1, M))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of differences of f-measure that is less than or equal to 0: 1000\n",
      "Percentage of differences of f-measure that is less than or equal to 0: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print('Number of differences of f-measure that is less than or equal to 0:',\n",
    "      np.sum(delta_f_measure <= 0))\n",
    "print('Percentage of differences of f-measure that is less than or equal to 0: {:.2f}%'.\n",
    "      format(np.sum(delta_f_measure <= 0)/M * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the boostrap test, logistic regression classifier is at least as good as lexicon-based one in terms of F-measure.\n",
    "\n",
    "##### Conclusion\n",
    "\n",
    "Thus, based on the results of the above tests, I would prefer logistic regression classifier to lexicon-based one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
